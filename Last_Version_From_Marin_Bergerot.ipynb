{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are you on PC or MAC? pc = 0, mac = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "computer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import glob as glob\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "\n",
    "matplotlib.rcParams[ 'figure.figsize' ] = ( 20, 6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOTPATH = os.getcwd()\n",
    "\n",
    "data_folder = 'DataSP2'\n",
    "\n",
    "# Path to the whole database\n",
    "path_data = os.path.join(ROOTPATH, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Importing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path, daterule):\n",
    "    '''\n",
    "    Format a Data frame with Adj Close prices for all tickers (columns) during the concerned date range (index).\n",
    "    Daterule item filters the number of dates we need\n",
    "    '''\n",
    "    list_dataframes = []\n",
    "    allFiles = glob.glob(path + \"/csv\" + \"*.csv\")\n",
    "    i = 0\n",
    "    list_tickers = []\n",
    "    \n",
    "    if computer == 0:\n",
    "        for file_ in allFiles:\n",
    "            with open(file_, 'r') as csvfile:\n",
    "                import_df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "                adj_close_arr = import_df['Adj Close'].values\n",
    "                ticker = file_.split('_')[3]\n",
    "                \n",
    "                if len(adj_close_arr) != daterule or np.isnan(adj_close_arr).any() or ticker == 'IBB' or ticker == '^GSPC':\n",
    "                    # If we don't have the specified number of obs, or some nan: ignore\n",
    "                    # Also ignore the IBB here. (Imported in Calibration)\n",
    "                    continue\n",
    "                    \n",
    "                list_dataframes = list_dataframes + list(adj_close_arr)\n",
    "                dates = import_df['Date'].values\n",
    "                list_tickers.append(ticker)\n",
    "                i += 1\n",
    "\n",
    "    if computer == 1:\n",
    "        for file_ in allFiles:\n",
    "            with open(file_, 'r', encoding ='mac_roman') as csvfile:\n",
    "                import_df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "                adj_close_arr = import_df['Adj Close'].values\n",
    "                ticker = file_.split('_')[3]\n",
    "                \n",
    "                if len(adj_close_arr) != daterule or np.isnan(adj_close_arr).any() or ticker == 'IBB'or ticker == '^GSPC':\n",
    "                    # If we don't have the specified number of obs, or some nan: ignore\n",
    "                    # Also ignore the IBB here. (Imported in Calibration)\n",
    "                    continue\n",
    "                    \n",
    "                list_dataframes = list_dataframes + list(adj_close_arr)\n",
    "                dates = import_df['Date'].values\n",
    "                list_tickers.append(ticker)\n",
    "                i += 1\n",
    "    \n",
    "    num_files = i\n",
    "    # Reshape our list to an array with the good shape, ready to be fitted in a dataframe\n",
    "    list_dataframes = np.reshape(list_dataframes, [len(list_tickers), len(dates)]).T\n",
    "    df_out_adj_close = pd.DataFrame(list_dataframes, index=dates, columns=list_tickers)\n",
    "    \n",
    "    return num_files, df_out_adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    '''\n",
    "    Use the preprocessing object MinMaxScaler from sklearn library to normalize the data.\n",
    "    The out dataframe has the same index and column\n",
    "    '''\n",
    "    x = df.values \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_n = pd.DataFrame(x_scaled, index=df.index, columns=df.columns)\n",
    "    return df_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create the first Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create the two first dataframes:\n",
    "- Dataframes with all Adj Closes\n",
    "- Dataframe with the normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the DataFrames:\n",
    "- Columns: Tickers (x155)\n",
    "- Rows: Dates (671 days)\n",
    "We will also extract the list of tickers in the right order, plus the quote dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_files, DF_adj_close = get_data(path_data, 671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_tickers is array_type\n",
    "list_tickers = DF_adj_close.columns\n",
    "# dates is array-type\n",
    "dates = DF_adj_close.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create for each ticker a dataframe of normalized adj close : DF_adj_close_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_adj_close_n = normalize_data(DF_adj_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoding Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Intialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(num_input, num_output):\n",
    "    weight_init = tf.random_normal_initializer(stddev=1.0/np.sqrt(num_input))\n",
    "    weight_shape = [num_input, num_output]\n",
    "    W = tf.get_variable('W', weight_shape, initializer = weight_init)\n",
    "    return(W)\n",
    "\n",
    "def bias_variable(num_output):\n",
    "    bias_init = tf.constant_initializer(value = 0)\n",
    "    bias_shape = [num_output]\n",
    "    b = tf.get_variable('b', bias_shape, initializer = bias_init)\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input_tensor, num_input, num_output, phase_train, layer_name, bool_drop):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        with tf.variable_scope('weights'):\n",
    "            W = weight_variable(num_input, num_output)\n",
    "            variable_summary_light(W)\n",
    "    \n",
    "        with tf.variable_scope('biases'):\n",
    "            b = bias_variable(num_output)\n",
    "            variable_summary_light(W)\n",
    "            \n",
    "        \n",
    "        if bool_drop:\n",
    "            dropped_or_not = dropout(input_tensor, keep_prob)\n",
    "        else:\n",
    "            dropped_or_not = input_tensor\n",
    "            \n",
    "        logits = tf.matmul(dropped_or_not, W) + b\n",
    "            \n",
    "    return tf.nn.tanh(layer_batch_normalization(logits, num_output, phase_train)), W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(input_tensor, n_code, phase_train):\n",
    "    with tf.variable_scope(\"encode\"):\n",
    "        \n",
    "        with tf.variable_scope('h_1'):\n",
    "            h_1, W_e_1 = layer(input_tensor, num_input, num_encode_1, phase_train, \"encoder\", False)\n",
    "            \n",
    "        with tf.variable_scope('output'):\n",
    "            output, W_e_2 = layer(h_1, num_encode_1, n_code, phase_train, \"encoder\",True)\n",
    "            \n",
    "    return output, W_e_2\n",
    "\n",
    "def decoder(input_tensor, n_code, phase_train):\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        \n",
    "        with tf.variable_scope('h_1'):\n",
    "            h_1, W_d_1 = layer(input_tensor, n_code, num_decode_1, phase_train, \"encoder\",True)\n",
    "        \n",
    "        with tf.variable_scope('output'):\n",
    "            output, W_d_2 = layer(h_1, num_decode_1, num_output, phase_train, \"decoder\",False)\n",
    "            \n",
    "    return output, W_d_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Build of the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part includes the following functions:\n",
    "- Dropout\n",
    "- MSE\n",
    "- Regularizer\n",
    "- Loss (MSE + Regularizer)\n",
    "- Training\n",
    "- Evaluate (write a summary for Tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout(encoder_op, keep_prob):\n",
    "    with tf.variable_scope(\"dropped\"):\n",
    "        dropped = tf.nn.dropout(encoder_op, keep_prob)\n",
    "    return dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(output, x):\n",
    "    with tf.variable_scope('mse'):\n",
    "        mse = tf.squared_difference(output, x)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularizer(W_e, W_d):\n",
    "    with tf.variable_scope(\"regularizer\"):\n",
    "        regularizer = tf.nn.l2_loss(W_e) + tf.nn.l2_loss(W_d)\n",
    "    return regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(mse, regularizer, lambd, num_input, batch_size):\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = tf.reduce_sum(mse + lambd * regularizer)/(num_input * batch_size * 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, x):\n",
    "    with tf.variable_scope('validation'):\n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x, name=\"val_diff\")), 1))\n",
    "        val_loss = tf.reduce_mean(l2_norm)\n",
    "        val_summary_op = tf.summary.scalar(\"val_cost\", val_loss)\n",
    "    return val_loss, val_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Tensorboard functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part includes the following functions:\n",
    "- Light summary of a variable\n",
    "- Comprehensive summary of a variable\n",
    "- Summary of the training: loss, input, latent (coding), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summary_light(var):\n",
    "    with tf.variable_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summary(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_summaries(loss, x, latent, output):\n",
    "    writer = tf.summary.FileWriter(\"./logs\")\n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    layer_grid_summary(\"Input\", x, [28, 28])\n",
    "    layer_grid_summary(\"Encoder\", latent, [2, 1])\n",
    "    layer_grid_summary(\"Output\", output, [28, 28])\n",
    "    return writer, tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6. Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch_ae(batch_size, data):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. Cut in terms of tickers\n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "\n",
    "    return np.array(data_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_batch_normalization(x, n_out, phase_train):\n",
    "\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    \n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        \n",
    "    mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    \n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : The next_fold function has been withdrawn, since there is no more cross-validation for the auto-encoding part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. Fetch data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_ticker_batch(data_norm, true, list_tickers):\n",
    "    '''\n",
    "    Retrieve a ticker from a normalized dataframe\n",
    "    '''\n",
    "    index = -1\n",
    "    i = 0\n",
    "    while index < 0:\n",
    "        if data_norm[i][100] - true[100] == 0:\n",
    "            index = i\n",
    "        i += 1\n",
    "    ticker = list_tickers[index]\n",
    "    return(ticker, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.0004\n",
    "num_steps = 5000\n",
    "batch_size = 30\n",
    "lambd = 0.12\n",
    "val_dropout = 0.89\n",
    "\n",
    "display_step = 1000\n",
    "\n",
    "# Network Parameters \n",
    "\n",
    "num_input = 503 # 503 days considered\n",
    "num_encode_1 = 250 # 1st layer of encode\n",
    "n_code = 5  # bottleneck layer\n",
    "num_decode_1 = 250 # 1st layer of decode\n",
    "num_output = num_input\n",
    "\n",
    "\n",
    "date = str(int(time()))\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/autoencoder1/'+date+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Format the dataframe of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just get the array of the normalized Adj Close of 155 tickers during the first 503 days. We mean not to touch the remaining days because of the test of the calibration method at the end of the study.\n",
    "\n",
    "Remark: We need to transpose the array so that X_ae[i] gives the 503 quotes of ticker number i (better for the training/testing process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ae = DF_adj_close_n.iloc[range(503),:].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This part needs to be adapted with the new syntax (not the former messy one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"autoencoder_model\"):\n",
    "        \n",
    "        # Placeholders\n",
    "        x = tf.placeholder(\"float\", [None, num_input])\n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "        keep_prob = tf.placeholder(tf.float32) #gateway for dropout\n",
    "        sample_size = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Extract the latent information (logits from the bottleneck layer)\n",
    "        code, W_e = encoder(x, n_code, phase_train)\n",
    "\n",
    "        # Extract the outputs of the autoencoder\n",
    "        output, W_d = decoder(code, n_code, phase_train)\n",
    "\n",
    "        cost = loss(mse(output,x), regularizer(W_e, W_d), lambd, num_input, sample_size)\n",
    "\n",
    "        train_op = training(cost, learning_rate)\n",
    "\n",
    "        eval_op, val_summary_op = evaluate(output, x)\n",
    "\n",
    "        # Merge all the summaries - variables, training - into a single operation\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # Save the trained model for later use (testing)\n",
    "        saver = tf.train.Saver()        \n",
    "\n",
    "        # Create the session list that will be runned for training\n",
    "        total_loss = np.zeros(num_steps)\n",
    "\n",
    "        start_time = time()\n",
    "                    \n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Initialize a summary for our variables during the training\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "        # Time to launch the session\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        for i in range(1, num_steps + 1):\n",
    "\n",
    "            # Number of loops = total tickers / batch_size\n",
    "            total_batch = int(len(list_tickers) / batch_size)\n",
    "\n",
    "            for j in range(total_batch):\n",
    "                mini_batch_x = next_batch_ae(batch_size, X_ae)\n",
    "                \n",
    "                _, loss_batch, summary = sess.run([train_op, cost, summary_op], \n",
    "                            feed_dict={x: mini_batch_x, phase_train: True, keep_prob: val_dropout, sample_size: batch_size})\n",
    "            \n",
    "            total_loss[i - 1] = loss_batch\n",
    "\n",
    "            # Write on the log file the datas for TensorBoard / First: summary, Second: overall step\n",
    "            summary_writer.add_summary(summary, i)\n",
    "\n",
    "            if i % display_step == 0:\n",
    "\n",
    "                print('Step %i: Minibatch Loss: %f' % (i, loss_batch))\n",
    "                curr_time = time()\n",
    "                elapsed_time = curr_time - start_time\n",
    "                print('Estimated remaining time = ', elapsed_time / i * (num_steps - i),'s\\n')\n",
    "\n",
    "        # Now get the loss vector of all the tickers, one by one, in the same order than tickers_import: l.\n",
    "        # Also get the predictions for plotting purposes: p.\n",
    "        loss_all , predic_n = sess.run([cost, output] , feed_dict={x: X_ae, phase_train: False, keep_prob: 1, sample_size: X_ae.shape[0]})\n",
    "        \n",
    "        print('Test Loss: '+ str(loss_all) + '\\n\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Performances of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decrease of loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_loss )\n",
    "plt.title('Variations of the loss during the training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a random normalized input and its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = np.where(list_tickers.values == 'PRU')[0][0]\n",
    "# Real value\n",
    "plt.plot(X_ae[plot_idx], label='Real Value')\n",
    "# Prediction\n",
    "plt.plot(predic_n[plot_idx], label='Prediction')\n",
    "plt.title('Prediction of ' + str(list_tickers[plot_idx]))\n",
    "plt.legend()\n",
    "plt.savefig('Prediction of most commonal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = np.where(list_tickers.values == 'HCA')[0][0]\n",
    "# Real value\n",
    "plt.plot(X_ae[plot_idx], label='Real Value')\n",
    "# Prediction\n",
    "plt.plot(predic_n[plot_idx], label='Prediction')\n",
    "plt.title('Prediction of ' + str(list_tickers[plot_idx]))\n",
    "plt.legend()\n",
    "plt.savefig('Prediction of least commonal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = np.random.randint(low=0, high=len(X_ae))\n",
    "# Real value\n",
    "plt.plot(X_ae[plot_idx], label='Real Value')\n",
    "# Prediction\n",
    "plt.plot(predic_n[plot_idx], label='Prediction')\n",
    "plt.title('Prediction of ' + str(list_tickers[plot_idx]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_tickers) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Finding the tickers of the best commonals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the commonals function (visualization of the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_commonals(pred_values, real_values, tickers):\n",
    "    '''\n",
    "    Warning: the pred_values, real_values and tickers must be sorted the same way!\n",
    "    Create a bar plot of the MSE of the predictions over their real value.\n",
    "    '''\n",
    "    \n",
    "    # Compute the array of the mean squared errors\n",
    "    MSE_arr = ((pred_values - real_values)**2).mean(axis=1)\n",
    "    \n",
    "    MSE_df = pd.DataFrame(MSE_arr, index = tickers, columns=['MSE'])\n",
    "    MSE_df = MSE_df.sort_values('MSE')\n",
    "    \n",
    "    indexes = MSE_df.index\n",
    "    cutted_indexes = [i for i in indexes]\n",
    "    for i in range(len(cutted_indexes)):\n",
    "        if i%5 != 0 and i != 0 and i!= len(cutted_indexes)-1:\n",
    "            cutted_indexes[i] = ''\n",
    "    for i in range(2,5):\n",
    "        cutted_indexes[-i] = ''\n",
    "    MSE_df = pd.DataFrame(MSE_df.values, index = cutted_indexes, columns=['MSE'])\n",
    "    \n",
    "    MSE_df_test = MSE_df[:]\n",
    "    MSE_df_test.plot.bar()\n",
    "    plt.title('Commonals')\n",
    "    plt.savefig('List of all commonals')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the most/least commonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_commonals(pred_values, real_values, tickers, S): \n",
    "    '''\n",
    "    Create the same dataframe than in the plot_commonals function\n",
    "    Same warning about sorting the arrays the same way.\n",
    "    Return a Data Frame with the MSE of the 10 most commonals and the S-10 least commonals\n",
    "    '''\n",
    "    MSE_arr = ((pred_values - real_values)**2).mean(axis=1)\n",
    "    \n",
    "    MSE_df = pd.DataFrame(MSE_arr, index = tickers, columns=['MSE'])\n",
    "    MSE_df = MSE_df.sort_values('MSE')\n",
    "    \n",
    "    # Take 10 with lowest MSE (most commonals)\n",
    "    most_c_df = MSE_df.sort_values('MSE', ascending=True).iloc[:10,:]\n",
    "    # Take S-10 with highest MSE (least commonals)\n",
    "    least_c_df = MSE_df.sort_values('MSE', ascending=False).iloc[:(S-10),:]\n",
    "    # Join\n",
    "    most_least_c_df = pd.concat([most_c_df, least_c_df]) \n",
    "    \n",
    "    return(most_least_c_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Plot the commonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_commonals(predic_n, X_ae, list_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Extract the 10 most | S-10 least commonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = 100\n",
    "\n",
    "most_least_c_df = return_commonals(predic_n, X_ae, list_tickers, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calibration Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the benchmark\n",
    "file_benchmark = path_data + \"/csv_^GSPC_from_2015-05-05_to_2018-01-01.csv\"\n",
    "\n",
    "if computer == 0:\n",
    "    with open(file_benchmark, 'r') as csvfile:\n",
    "        import_df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "        benchmark = import_df['Adj Close']\n",
    "        benchmark.index = import_df['Date']\n",
    "        \n",
    "if computer == 1:\n",
    "    with open(file_benchmark, 'r', encoding ='mac_roman') as csvfile:\n",
    "        import_df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "        benchmark = import_df['Adj Close']\n",
    "        benchmark.index = import_df['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Operating functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to match the log rate of return of the index with the log rate of returns of our 10 most | S-10 least commonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logRateOfReturn(df, shift):\n",
    "    \n",
    "    # Making sure dataframe is well sorted\n",
    "    df_sorted = df.sort_index(ascending = True)\n",
    "    \n",
    "    #Be careful with the shift!\n",
    "    df_shifted = df_sorted.shift(shift)\n",
    "    log_rate_of_returns = np.log(df_sorted / df_shifted)\n",
    "    for i in range(shift):\n",
    "        log_rate_of_returns = log_rate_of_returns.drop(log_rate_of_returns.index[0])\n",
    "\n",
    "    return log_rate_of_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to modify the benchmark so that we set +threshold% instead of drawdown of -threshold% or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_benchmark(benchmark, threshold, shift):\n",
    "    '''\n",
    "    We only import the Adj Close column from the benchmark, so the benchmark is a Series\n",
    "    '''\n",
    "    rate = logRateOfReturn(benchmark,shift)\n",
    "    for i in range(len(rate)):\n",
    "        if rate[i] <= - threshold:\n",
    "            rate[i] = threshold\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purposes, we want to recompose the evolution of $P0 invested, based on the log rate of returns that we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(rates, P0):\n",
    "    '''\n",
    "    rates is an array type variable\n",
    "    '''\n",
    "    prices = [P0]\n",
    "    for i in range(len(rates)):\n",
    "        prices.append( np.exp(rates[i]) * prices[i])\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-use the layer function introduced in the Auto-Encoder part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer1_calib(input_tensor, S, n_code, phase_train):\n",
    "    with tf.variable_scope(\"calib\"):\n",
    "            \n",
    "        with tf.variable_scope('output'):\n",
    "            output, W_e = layer(input_tensor, S, n_code, phase_train, \"encoder\", True)\n",
    "            \n",
    "    return output, W_e\n",
    "\n",
    "def layer2_calib(input_tensor, S, n_code, phase_train):\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        \n",
    "        with tf.variable_scope('output'):\n",
    "            output, W_d = layer(input_tensor, n_code, S, phase_train, \"decoder\", False)\n",
    "    \n",
    "\n",
    "    return tf.nn.softmax(output), W_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch_calib(batch_size, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. Cut in terms of tickers\n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.array(data_shuffle), np.array(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Format the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the daily log returns of the benchmark, and the modified benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 day shift\n",
    "shift = 1\n",
    "threshold = 0.03\n",
    "\n",
    "benchmark_lrets = logRateOfReturn(benchmark, shift)\n",
    "benchmark_lrets_mod = modify_benchmark(benchmark, threshold, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_lrets.plot(label='Real Adj Close')\n",
    "benchmark_lrets_mod.plot(label='Modified')\n",
    "plt.title('Comparison between benchmark and modified benchmark')\n",
    "plt.legend()\n",
    "plt.savefig('Comparison between benchmark and modified benchmark in terms of ror')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_returns_in = reconstruct(benchmark_lrets_mod[:503].values,1)\n",
    "real_returns_in = reconstruct(benchmark_lrets[:503].values,1)\n",
    "plt.plot(calib_returns_in,'r', label = \"Objective\")\n",
    "plt.plot(real_returns_in,'y', label = \"Benchmark\")\n",
    "plt.title('Comparison between benchmark and modified benchmark')\n",
    "plt.legend()\n",
    "plt.savefig('Comparison between benchmark and modified benchmark in terms of prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the dataframe of the selected portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tickers_ptfolio = most_least_c_df.index\n",
    "DF_most_least_c = DF_adj_close.loc[:,tickers_ptfolio]\n",
    "DF_ptfolio_lrets = logRateOfReturn(DF_most_least_c, shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate_calib = 0.0004\n",
    "num_steps_calib = 1000\n",
    "batch_size_calib = 10\n",
    "lambd_calib = 0.1\n",
    "val_dropout_calib = 0.88\n",
    "\n",
    "display_step_calib = 1\n",
    "\n",
    "\n",
    "\n",
    "# Network Parameters \n",
    "\n",
    "num_input_calib = S\n",
    "n_hidden_calib = 5  # bottleneck layer\n",
    "num_output_calib = S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. Format the input/output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we extract only the values of the lrets in the same orders to retrieve the tickers easily. The train is composed by the 503 first days, the test on the remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_c_train = DF_ptfolio_lrets.iloc[range(503),:].values\n",
    "X_c_test = DF_ptfolio_lrets.iloc[np.arange(503,670),:].values\n",
    "Y_c_train = benchmark_lrets_mod[:503].values\n",
    "Y_c_test = benchmark_lrets_mod[503:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"calibration_phase\"):\n",
    "        \n",
    "        # Placeholders\n",
    "        x = tf.placeholder(\"float\", [None, S])\n",
    "        label = tf.placeholder(\"float\", [None, 1])\n",
    "        \n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "        keep_prob = tf.placeholder(tf.float32) #gateway for dropout\n",
    "        sample_size = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Extract the latent information (logits from the bottleneck layer)\n",
    "        hidden, W_e = layer1_calib(x, S, n_hidden_calib, phase_train)\n",
    "\n",
    "        # Extract the outputs of the autoencoder\n",
    "        output, W_d = layer2_calib(hidden, S, n_hidden_calib, phase_train)\n",
    "        \n",
    "        rr_tot = np.dot(output, x)\n",
    "            \n",
    "        cost = loss(mse(rr_tot, label), regularizer(W_e, W_d), lambd_calib, num_input_calib, sample_size)\n",
    "\n",
    "        train_op = training(cost, learning_rate_calib)\n",
    "\n",
    "        eval_op, val_summary_op = evaluate(rr_tot, label)\n",
    "\n",
    "        # Merge all the summaries - variables, training - into a single operation\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # Save the trained model for later use (testing)\n",
    "        saver = tf.train.Saver()        \n",
    "\n",
    "        # Create the session list that will be runned for training\n",
    "        \n",
    "        total_loss = np.zeros(num_steps_calib)\n",
    "        \n",
    "        start_time = time()\n",
    "        \n",
    "        session = tf.Session()\n",
    "        \n",
    "        init_op = tf.global_variables_initializer()\n",
    "        session.run(init_op)\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        \n",
    "        for i in range(1, num_steps_calib + 1):\n",
    "            \n",
    "            total_batch = int(len(X_c_train) / batch_size_calib)\n",
    "\n",
    "            for j in range(total_batch):\n",
    "                mini_batch_x, mini_batch_y = next_batch_calib(batch_size_calib, X_c_train, Y_c_train)\n",
    "\n",
    "                _, loss_batch, summary = session.run([train_op, cost, summary_op], \n",
    "                            feed_dict={x: mini_batch_x , label: mini_batch_y.reshape((batch_size_calib,1)), \n",
    "                                       phase_train: True, keep_prob: val_dropout_calib, sample_size:  batch_size_calib})\n",
    "            \n",
    "            \n",
    "            total_loss[i-1] = loss_batch\n",
    "            \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            \n",
    "            if i % display_step == 0:\n",
    "\n",
    "                    print('Step %i : Minibatch Loss: %f' % (i, loss_batch))\n",
    "                    curr_time = time()\n",
    "                    elapsed_time = curr_time-start_time\n",
    "                    print('Estimated remaining time = ', elapsed_time / i * (num_steps_calib-i),'s\\n')\n",
    "                    \n",
    "        # Get the weights at the end for the in sample data\n",
    "        predi_calib_in = session.run(output , feed_dict={x: X_c_train , label: Y_c_train.reshape((len(Y_c_train),1)), phase_train: False, keep_prob: 1, sample_size: X_c_train.shape[0]})\n",
    "        \n",
    "        # Get the weights at the end for the out of sample data\n",
    "        predi_calib_out = session.run(output , feed_dict={x: X_c_test , label: Y_c_test.reshape((len(Y_c_test),1)), phase_train: False, keep_prob: 1, sample_size: X_c_test.shape[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Analyse the performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_loss)\n",
    "plt.title('Variations of the loss during the training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi_calib_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi_calib_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = len(predi_calib_in)\n",
    "print(n_in)\n",
    "\n",
    "weights_ptfolio2_in = [0]*S\n",
    "\n",
    "for i in range(S):\n",
    "    for j in predi_calib_in:\n",
    "\n",
    "        weights_ptfolio2_in[i] += j[i]/n_in\n",
    "print(weights_ptfolio2_in[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = len(predi_calib_out)\n",
    "print(n_out)\n",
    "\n",
    "weights_ptfolio2_out = [0]*S\n",
    "\n",
    "for i in range(S):\n",
    "    for j in predi_calib_out:\n",
    "\n",
    "        weights_ptfolio2_out[i] += j[i]/n_out\n",
    "print(weights_ptfolio2_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the weight\n",
    "weights_ptfolio_in = predi_calib_in[0]\n",
    "weights_ptfolio_out = predi_calib_out[0]\n",
    "\n",
    "# Cumulate the returns of the portfolio over the period of time in test\n",
    "ptfolio_lrets_in = np.dot(X_c_train, weights_ptfolio2_in)\n",
    "ptfolio_lrets_out = np.dot(X_c_test, weights_ptfolio2_out)\n",
    "\n",
    "real_benchmark_lrets_test = benchmark_lrets[503:].values\n",
    "real_benchmark_lrets_train = benchmark_lrets[:503].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ptfolio_lrets_in,'b', label = \"Prediction\")\n",
    "plt.plot([np.mean(ptfolio_lrets_in)]*len(ptfolio_lrets_in),'b-.')\n",
    "plt.plot(Y_c_train,'r', label = \"Objective\")\n",
    "plt.plot([np.mean(Y_c_train)]*len(Y_c_train),'r-.')\n",
    "plt.legend()\n",
    "plt.title('Comparison of the log returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ptfolio_lrets_out,'b', label = \"Prediction\")\n",
    "plt.plot([np.mean(ptfolio_lrets_out)]*len(ptfolio_lrets_out),'b-.')\n",
    "plt.plot(Y_c_test,'r', label = \"Objective\")\n",
    "plt.plot([np.mean(Y_c_test)]*len(Y_c_test),'r-.')\n",
    "plt.legend()\n",
    "plt.title('Comparison of the log returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the reconstructed curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns_in = reconstruct(ptfolio_lrets_in,1)\n",
    "calib_returns_in = reconstruct(Y_c_train,1)\n",
    "real_returns_in = reconstruct(real_benchmark_lrets_train,1)\n",
    "plt.plot(portfolio_returns_in,'b', label = \"Prediction\")\n",
    "plt.plot(calib_returns_in,'r', label = \"Objective\")\n",
    "plt.plot(real_returns_in,'y', label = \"Benchmark\")\n",
    "plt.title('Comparison of the returns')\n",
    "plt.legend()\n",
    "plt.savefig('Comparison of the returns in sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns_out = reconstruct(ptfolio_lrets_out,1)\n",
    "calib_returns_out = reconstruct(Y_c_test,1)\n",
    "real_returns_out = reconstruct(real_benchmark_lrets_test,1)\n",
    "plt.plot(portfolio_returns_out,'b', label = \"Prediction\")\n",
    "plt.plot(calib_returns_out,'r', label = \"Objective\")\n",
    "plt.plot(real_returns_out,'y', label = \"Benchmark\")\n",
    "plt.title('Comparison of the returns')\n",
    "plt.legend()\n",
    "plt.savefig('Comparison of the returns out of sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Export a dataframe for Quantopian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Create the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe will be structured as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   date  | symbol | weight | price |\n",
    "|---------|--------|--------|-----------|\n",
    "| 671 obs |   AA   |   0.1  |    28.7   |\n",
    "|   ...   |  ...   |   ...  |    ...    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_days_in = len(Y_c_train)\n",
    "out_df_in = pd.DataFrame(index=np.arange(nb_days_in*S), columns=['date', 'symbol', 'weight', 'price'])\n",
    "\n",
    "nb_days_out = len(Y_c_test)\n",
    "out_df_out = pd.DataFrame(index=np.arange(nb_days_out*S), columns=['date', 'symbol', 'weight', 'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CXO : Done\n",
      "NI : Done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-62f5e07b3f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_days\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0marr_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtickers_ptfolio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_ptfolio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDF_most_least_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mout_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnb_days\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers_ptfolio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[1;32m--> 639\u001b[1;33m                                                     value=value)\n\u001b[0m\u001b[0;32m    640\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36msetitem\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3440\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'setitem'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3328\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3329\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3330\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36msetitem\u001b[1;34m(self, indexer, value, mgr)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[1;31m# coerce and try to infer the dtypes of the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_coerce_and_cast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_try_coerce_and_cast_result\u001b[1;34m(self, result, dtype)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_coerce_and_cast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_coerce_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_cast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_try_cast_result\u001b[1;34m(self, result, dtype)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# may need to change the dtype here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_downcast_to_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_coerce_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mmaybe_downcast_to_dtype\u001b[1;34m(result, dtype)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0minferred_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ensure_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minferred_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bool'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(S):\n",
    "    for j in range(nb_days_in):\n",
    "        arr_temp = [dates[j], tickers_ptfolio[i], weights_ptfolio2_in[i], DF_most_least_c.iloc[j, i]]\n",
    "        out_df_in.iloc[i * nb_days_in + j, :] = arr_temp\n",
    "    print(tickers_ptfolio[i], ': Done')\n",
    "    \n",
    "for i in range(S):\n",
    "    for j in range(nb_days_in,nb_days_out+nb_days_out):\n",
    "        arr_temp = [dates[j], tickers_ptfolio[i], weights_ptfolio2_out[i], DF_most_least_c.iloc[j, i]]\n",
    "        out_df_out.iloc[i * nb_days_out + j, :] = arr_temp\n",
    "    print(tickers_ptfolio[i], ': Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Export the dataframe to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not take into account the index (just a range of integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'Weigths_in.csv', 'a' ) as f:\n",
    "    out_df_in.to_csv( f, index=False )\n",
    "    \n",
    "with open( 'Weigths_out.csv', 'a' ) as f:\n",
    "    out_df_out.to_csv( f, index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
