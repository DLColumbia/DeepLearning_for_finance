{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "import csv\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#auto_encoder_v1 should be located in the same folder as the folder DeepLearning_for_finance-master\n",
    "#within DeepLearning_for_finance-master folder you have the folder Data and the file IBB_holdings.csv\n",
    "os.chdir('/Users/NicoTcht/Desktop/ColumbiaUniv/Spring18/deep_Learning/project/DeepLearning_for_finance-master/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABUS',\n",
       " 'ACAD',\n",
       " 'ACHN',\n",
       " 'ACOR',\n",
       " 'ACRS',\n",
       " 'ADAP',\n",
       " 'ADMS',\n",
       " 'ADRO',\n",
       " 'AERI',\n",
       " 'AGIO',\n",
       " 'AIMT',\n",
       " 'AKAO',\n",
       " 'AKBA',\n",
       " 'AKCA',\n",
       " 'ALDR',\n",
       " 'ALKS',\n",
       " 'ALNY',\n",
       " 'ALXN',\n",
       " 'AMAG',\n",
       " 'AMGN',\n",
       " 'AMPH',\n",
       " 'AMRN',\n",
       " 'ANAB',\n",
       " 'ANIP',\n",
       " 'ARDX',\n",
       " 'ARNA',\n",
       " 'ARRY',\n",
       " 'ARWR',\n",
       " 'ASND',\n",
       " 'ATNX',\n",
       " 'ATRA',\n",
       " 'AUPH',\n",
       " 'AVDL',\n",
       " 'AVXS',\n",
       " 'AXON',\n",
       " 'BCRX',\n",
       " 'BGNE',\n",
       " 'BIIB',\n",
       " 'BIVV',\n",
       " 'BLCM',\n",
       " 'BLUE',\n",
       " 'BMRN',\n",
       " 'BOLD',\n",
       " 'BPMC',\n",
       " 'CALA',\n",
       " 'CARA',\n",
       " 'CASC',\n",
       " 'CBPO',\n",
       " 'CCXI',\n",
       " 'CELG',\n",
       " 'CERS',\n",
       " 'CHRS',\n",
       " 'CLDX',\n",
       " 'CLLS',\n",
       " 'CLVS',\n",
       " 'CLXT',\n",
       " 'CMRX',\n",
       " 'CNCE',\n",
       " 'COLL',\n",
       " 'CORI',\n",
       " 'CRBP',\n",
       " 'CRIS',\n",
       " 'CRSP',\n",
       " 'CRVS',\n",
       " 'CTMX',\n",
       " 'CYTK',\n",
       " 'DBVT',\n",
       " 'DEPO',\n",
       " 'DERM',\n",
       " 'DOVA',\n",
       " 'ECYT',\n",
       " 'EDIT',\n",
       " 'EGRX',\n",
       " 'ENDP',\n",
       " 'ENTA',\n",
       " 'EPZM',\n",
       " 'ESPR',\n",
       " 'EXEL',\n",
       " 'FGEN',\n",
       " 'FLXN',\n",
       " 'FMI',\n",
       " 'FOLD',\n",
       " 'FOMX',\n",
       " 'FPRX',\n",
       " 'GBT',\n",
       " 'GERN',\n",
       " 'GHDX',\n",
       " 'GILD',\n",
       " 'GLPG',\n",
       " 'GLYC',\n",
       " 'GRFS',\n",
       " 'GTHX',\n",
       " 'GWPH',\n",
       " 'HALO',\n",
       " 'HZNP',\n",
       " 'ICPT',\n",
       " 'ILMN',\n",
       " 'IMGN',\n",
       " 'IMMU',\n",
       " 'INCY',\n",
       " 'INO',\n",
       " 'INSM',\n",
       " 'INSY',\n",
       " 'INVA',\n",
       " 'IONS',\n",
       " 'IOVA',\n",
       " 'IRWD',\n",
       " 'ITCI',\n",
       " 'JAZZ',\n",
       " 'JNCE',\n",
       " 'JUNO',\n",
       " 'KALA',\n",
       " 'KPTI',\n",
       " 'KURA',\n",
       " 'LGND',\n",
       " 'LMNX',\n",
       " 'LOXO',\n",
       " 'LXRX',\n",
       " 'MCRB',\n",
       " 'MDCO',\n",
       " 'MEDP',\n",
       " 'MGNX',\n",
       " 'MNKD',\n",
       " 'MNTA',\n",
       " 'MRNS',\n",
       " 'MRSN',\n",
       " 'MYGN',\n",
       " 'MYL',\n",
       " 'MYOK',\n",
       " 'NBIX',\n",
       " 'NBRV',\n",
       " 'NEOS',\n",
       " 'NERV',\n",
       " 'NH',\n",
       " 'NK',\n",
       " 'NKTR',\n",
       " 'NLNK',\n",
       " 'NSTG',\n",
       " 'NTLA',\n",
       " 'NVAX',\n",
       " 'NVCR',\n",
       " 'OMER',\n",
       " 'ONCE',\n",
       " 'OPK',\n",
       " 'PACB',\n",
       " 'PBYI',\n",
       " 'PCRX',\n",
       " 'PDLI',\n",
       " 'PETQ',\n",
       " 'PETX',\n",
       " 'PGNX',\n",
       " 'PRAH',\n",
       " 'PRTA',\n",
       " 'PRTK',\n",
       " 'PTCT',\n",
       " 'PTLA',\n",
       " 'QURE',\n",
       " 'RARE',\n",
       " 'RARX',\n",
       " 'RDUS',\n",
       " 'REGN',\n",
       " 'RETA',\n",
       " 'RGEN',\n",
       " 'RGNX',\n",
       " 'RIGL',\n",
       " 'RTRX',\n",
       " 'RVNC',\n",
       " 'SAGE',\n",
       " 'SBBP',\n",
       " 'SCMP',\n",
       " 'SGEN',\n",
       " 'SGMO',\n",
       " 'SGYP',\n",
       " 'SHPG',\n",
       " 'SNDX',\n",
       " 'SNNA',\n",
       " 'SPPI',\n",
       " 'SRPT',\n",
       " 'SUPN',\n",
       " 'SVRA',\n",
       " 'SYNH',\n",
       " 'TBPH',\n",
       " 'TECH',\n",
       " 'TLGT',\n",
       " 'TOCA',\n",
       " 'TSRO',\n",
       " 'TTPH',\n",
       " 'TXMD',\n",
       " 'UTHR',\n",
       " 'VCYT',\n",
       " 'VNDA',\n",
       " 'VRTX',\n",
       " 'VTL',\n",
       " 'VYGR',\n",
       " 'XLRN',\n",
       " 'XNCR',\n",
       " 'ZGNX']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curdir = os.getcwd()\n",
    "\n",
    "file = curdir + '/IBB_holdings.csv'\n",
    "\n",
    "# Please remove the encoding = 'mac_roman' paramater if you run this file with Windows\n",
    "with open(file, 'r', encoding ='mac_roman') as csvfile:\n",
    "    file = csv.reader(csvfile,delimiter=' ')\n",
    "    c=0\n",
    "    list_tickers=[]\n",
    "    for row in file:\n",
    "        if c>=11:\n",
    "            list_tickers.append(row[0].split(',')[0])\n",
    "        c+=1\n",
    "            \n",
    "csvfile.close()\n",
    "list_tickers.sort()\n",
    "list_tickers.pop(-1)\n",
    "list_tickers.remove(\"BLKFDS\")\n",
    "list_tickers.remove(\"USD\")\n",
    "list_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure you are within the DeepLearning_for_finance-master folder\n",
    "os.chdir('/Users/NicoTcht/Desktop/ColumbiaUniv/Spring18/deep_Learning/project/DeepLearning_for_finance-master/')\n",
    "cur_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to add noise to a dataframe (a new csv will be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise_2(df_origin, tick, mean = 0, std = 0.005):\n",
    "    noise = []\n",
    "    \n",
    "    # Creating the noise\n",
    "    for i in range(len(df_origin)):\n",
    "        x = np.exp(np.random.normal(mean,std))\n",
    "        noise.append(x)\n",
    "        \n",
    "    df_modified = df_origin.copy()\n",
    "    df_modified['Adj Close'] = df_modified['Adj Close'].multiply(noise)\n",
    "    #New csv files are marked with _B_ standing for \"Bruit\"\n",
    "    new_name = 'csv_'+tick+'_B_'+str(mean)+'_'+str(std)+'_from_'+start_date+'_to_'+end_date+'.csv'\n",
    "    \n",
    "    os.chdir(cur_dir + '/Data')\n",
    "    \n",
    "    df_modified.to_csv(new_name , sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all the new csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please remove the comment within the loop if you want to create more input files. Warning: this code can be run only when your 197 csv files are in Data/. If you have already computed the code, it will return an error (because the list_tickers is only 197 long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/NicoTcht/Desktop/ColumbiaUniv/Spring18/deep_Learning/project/DeepLearning_for_finance-master/'\n",
    "allFiles = glob.glob(path + \"/Data/*.csv\")\n",
    "i=0\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Please remove the encoding = 'mac_roman' paramater if you run this file with Windows\n",
    "    with open(file_, 'r', encoding ='mac_roman') as csvfile:\n",
    "        df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "        add_noise_2(df,list_tickers[i],0,0.001)\n",
    "        #add_noise_2(df,list_tickers[i],0,0.008)\n",
    "        #add_noise_2(df,list_tickers[i],0,0.0003)\n",
    "        #add_noise_2(df,list_tickers[i],0,0.002)\n",
    "        #add_noise_2(df,list_tickers[i],0,0.0006)\n",
    "        i=i+1\n",
    "    csvfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tickers = len(list_tickers)\n",
    "list_dataframes=[None]*(2*nb_tickers)\n",
    "data = np.array([np.zeros(503)]*(2*nb_tickers-56)) \n",
    "\n",
    "\n",
    "allFiles = glob.glob(path + \"/Data/*.csv\")\n",
    "i = 0\n",
    "compt = 0\n",
    "for file_ in allFiles:\n",
    "    with open(file_, 'r', encoding ='mac_roman') as csvfile:\n",
    "        df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "        list_dataframes[i] = df\n",
    "        arr = np.array(df['Adj Close'])\n",
    "        if len(arr) == 503 and not df.isnull().values.any():    \n",
    "            #print(tick,i)\n",
    "            data[i] = arr/arr[0]-1\n",
    "            i += 1\n",
    "        else :\n",
    "            #print(np.shape(arr) ,tick)\n",
    "            compt += 1\n",
    "    csvfile.close()\n",
    "\n",
    "np.random.shuffle(data)    \n",
    "train_set = data[:-50]\n",
    "test_set = data[-50:-25]\n",
    "validation_set = data[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.0008\n",
    "num_steps = 100000\n",
    "batch_size = 20\n",
    "\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 250 # 1st layer num features\n",
    "num_hidden_2 = 100 # 2nd layer num features (the latent dim)\n",
    "num_input = 503 # MNIST data input (img shape: 28*28)\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss: 0.688654\n",
      "Step 1000: Minibatch Loss: 0.131300\n",
      "Step 2000: Minibatch Loss: 0.142536\n",
      "Step 3000: Minibatch Loss: 0.154722\n",
      "Step 4000: Minibatch Loss: 0.312811\n",
      "Step 5000: Minibatch Loss: 0.199298\n",
      "Step 6000: Minibatch Loss: 0.096957\n",
      "Step 7000: Minibatch Loss: 0.159180\n",
      "Step 8000: Minibatch Loss: 0.201710\n",
      "Step 9000: Minibatch Loss: 0.161201\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, _ = next_batch(batch_size,train_set,train_set)\n",
    "\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "    _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "    # Display logs per step\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        print('Step %i: Minibatch Loss: %f' % (i, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Loss:  0.240252\n",
      "Minibatch Loss:  0.230031\n",
      "Minibatch Loss:  0.168706\n",
      "Minibatch Loss:  0.220513\n",
      "Minibatch Loss:  0.154596\n",
      "Minibatch Loss:  0.134325\n",
      "Minibatch Loss:  0.167678\n",
      "Minibatch Loss:  0.172779\n",
      "Minibatch Loss:  0.234998\n",
      "Minibatch Loss:  0.206412\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Encode and decode images from test set and visualize their reconstruction.\n",
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "    # MNIST test set\n",
    "    batch_x, _ = next_batch(batch_size,test_set,test_set)\n",
    "    # Encode and decode the digit image)\n",
    "    l , p = sess.run([loss,y_pred] , feed_dict={X: batch_x})\n",
    "    print('Minibatch Loss: ', l)\n",
    "    \n",
    "    # Display original images\n",
    "    \n",
    "    # Display reconstructed images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
