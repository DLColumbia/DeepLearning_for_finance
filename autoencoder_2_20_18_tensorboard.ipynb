{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Are you on PC or MAC? pc = 0, mac = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer = 1\n",
    "#! source activate tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import glob as glob\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import time\n",
    "\n",
    "matplotlib.rcParams[ 'figure.figsize' ] = ( 14, 6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive way to set the path to Data directory\n",
    "\n",
    "ROOTPATH = os.getcwd()\n",
    "\n",
    "if not(ROOTPATH[-4:] == 'Data'):\n",
    "    path = os.path.join(ROOTPATH, 'Data')\n",
    "else:\n",
    "    path = ROOTPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the list of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if computer == 0:\n",
    "    with open('IBB_holdings.csv', 'r') as csvfile:\n",
    "        file = csv.reader(csvfile,delimiter=' ')\n",
    "        c=0\n",
    "        list_tickers=[]\n",
    "        for row in file:\n",
    "            if c>=11:\n",
    "                list_tickers.append(row[0].split(',')[0])\n",
    "            c+=1\n",
    "\n",
    "if computer == 1:\n",
    "    with open('IBB_holdings.csv', 'r', encoding ='mac_roman') as csvfile:\n",
    "        file = csv.reader(csvfile,delimiter=' ')\n",
    "        c=0\n",
    "        list_tickers=[]\n",
    "        for row in file:\n",
    "            if c>=11:\n",
    "                list_tickers.append(row[0].split(',')[0])\n",
    "            c+=1            \n",
    "\n",
    "list_tickers.sort()\n",
    "list_tickers.pop()\n",
    "list_tickers.remove(\"BLKFDS\")\n",
    "list_tickers.remove(\"USD\")\n",
    "list_tickers.remove(\"SNDX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data and convert into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tickers = len(list_tickers)\n",
    "list_dataframes=[]\n",
    "\n",
    "\n",
    "allFiles = glob.glob(path + \"/csv\" + \"*.csv\")\n",
    "i = 0\n",
    "\n",
    "# Fetch the csv data in the directory\n",
    "\n",
    "if computer == 0:\n",
    "    for file_ in allFiles:\n",
    "        with open(file_, 'r') as csvfile:\n",
    "            list_dataframes.append(pd.read_csv(csvfile, index_col=None, header=0))\n",
    "            i+=1\n",
    "\n",
    "if computer == 1:\n",
    "    for file_ in allFiles:\n",
    "        with open(file_, 'r', encoding ='mac_roman') as csvfile:\n",
    "            list_dataframes.append(pd.read_csv(csvfile, index_col=None, header=0))\n",
    "            i += 1\n",
    "            \n",
    "num_files = i\n",
    "            \n",
    "            \n",
    "maxi = 0\n",
    "length = 0\n",
    "compt = 0\n",
    "j = 0\n",
    "\n",
    "# Fetch the maximum length of the dfs\n",
    "\n",
    "for df in list_dataframes:\n",
    "    if len(df) > maxi:\n",
    "        maxi = len(df['Adj Close'])\n",
    "\n",
    "# Initialize the arrays that will stock the stock prices\n",
    "\n",
    "data = np.array([np.zeros(maxi)]*(num_files))\n",
    "data_normalized = np.array([np.zeros(maxi)]*(num_files))\n",
    "tickers_matching = []\n",
    "# Fill the arrays of the stocks and normalize the data in another array\n",
    "\n",
    "for i in range(len(list_dataframes)):\n",
    "    df = list_dataframes[i]\n",
    "    arr = np.array(df['Adj Close'])\n",
    "    if len(arr) == maxi and not df['Adj Close'].isnull().values.any(): # Null value of stock breaks the NN\n",
    "        # Tricky way to get the tickers in the same order as the datas\n",
    "        tickers_matching.append(''.join(x for x in allFiles[i][115:119] if x.upper()))\n",
    "        \n",
    "        data[j] = arr\n",
    "        minimum = np.min(arr)\n",
    "        maximum = np.max(arr)\n",
    "        arr = (arr - minimum) / (maximum - minimum)\n",
    "        data_normalized[j] = arr\n",
    "        j += 1\n",
    "    else :\n",
    "        compt += 1            \n",
    "    \n",
    "# Compt is giving the number of stock that we have filtered during the conversion process above\n",
    "# At last, compt=29 here\n",
    "data = data[:-compt]  \n",
    "data_normalized = data_normalized[:-compt]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training, validation and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the previous data\n",
    "idx = np.arange(0 , len(data))\n",
    "np.random.shuffle(idx)\n",
    "data_shuffle = [data_normalized[ i] for i in idx]\n",
    "\n",
    "\n",
    "# Train on approximately 75% of the dataset, validation on 12.5% and test on 12.5%\n",
    "train_set = data_shuffle[:-50]\n",
    "test_set = data_shuffle[-50:-25]\n",
    "validation_set = data_shuffle[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(num_input, num_output):\n",
    "    init = tf.random_normal([num_input, num_output],mean=0.0, stddev=1.0/np.sqrt(num_input))\n",
    "    return(tf.Variable(init))\n",
    "\n",
    "def bias_variable(num_output):\n",
    "    init = tf.zeros([num_output]),\n",
    "    return(tf.Variable(init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.array(data_shuffle), np.array(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary_light(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ticker_batch(batch, data_norm):\n",
    "    true = batch\n",
    "    index = -1\n",
    "    i = 0\n",
    "    while index < 0:\n",
    "        if data_norm[i][0] - true[0] == 0:\n",
    "            index = i\n",
    "        i += 1\n",
    "    ticker = tickers_matching[index]\n",
    "    return(ticker, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519146972\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 300\n",
    "batch_size = 10\n",
    "lambd = 0.1\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters \n",
    "\n",
    "num_input = 503 # 503 days considered\n",
    "num_hidden_1 = 250 # 1st layer num features\n",
    "num_output = num_input\n",
    "\n",
    "date = str(int(time()))\n",
    "print(date)\n",
    "logs_path = '/tmp/tensorflow_logs/autoencoder1/'+date+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Create NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "learning = tf.placeholder(\"float\")\n",
    "keep_prob = tf.placeholder(tf.float32) #gateway for dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1],mean=0.0,\n",
    "    stddev=1.0/np.sqrt(num_input))),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_1, num_input],mean=0.0,\n",
    "    stddev=1.0/np.sqrt(num_hidden_1)))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.zeros([num_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.zeros([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building the encoder\n",
    "def encoder(x, layer_name='encoder'):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights_e = weights['encoder_h1']\n",
    "            variable_summary(weights_e)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases_e = biases['encoder_b1']\n",
    "            variable_summary(biases_e)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.add(tf.matmul(x, weights_e), biases_e)\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = tf.nn.relu(preactivate)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "### Building the decoder\n",
    "def decoder(x,  layer_name='decoder'):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights_d = weights['decoder_h1']\n",
    "            variable_summary(weights_d)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases_d = biases['decoder_b1']\n",
    "            variable_summary(biases_d)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.add(tf.matmul(x, weights_d), biases_d)\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = tf.nn.relu(preactivate)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General way to implement layers with TensorBoard writing\n",
    "\n",
    "def encoder(input_tensor, input_dim, output_dim, layer_name='encoder'):\n",
    "    # Just 1 layer\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights_e = weight_variable(input_dim, output_dim)\n",
    "            variable_summary_light(weights_e)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases_e = bias_variable(output_dim)\n",
    "            variable_summary_light(biases_e)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights_e) + biases_e\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        with tf.name_scope('batch_norm'):\n",
    "            bn = tf.layers.batch_normalization(preactivate, training = True)\n",
    "            tf.summary.histogram('batch_normalization', bn)\n",
    "        activations = tf.nn.relu(bn)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return(activations, weights_e, biases_e)\n",
    "    \n",
    "def bottleneck(input_tensor, input_dim, output_dim, layer_name='bottleneck'):\n",
    "    # Just 1 layer / return the latent \n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights_b = weight_variable(input_dim, output_dim)\n",
    "            variable_summary_light(weights_b)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases_b = bias_variable(output_dim)\n",
    "            variable_summary_light(biases_b)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights_b) + biases_b\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        with tf.name_scope('batch_norm'):\n",
    "            bn = tf.layers.batch_normalization(preactivate, training = True)\n",
    "            tf.summary.histogram('batch_normalization', bn)\n",
    "        activations = tf.nn.relu(bn)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return(activations, weights_b, biases_b, preactivate)\n",
    "\n",
    "def decoder(input_tensor, input_dim, output_dim, layer_name='decoder'):\n",
    "    # Just 1 layer\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights_d = weight_variable(input_dim, output_dim)\n",
    "            variable_summary_light(weights_d)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases_d = bias_variable(output_dim)\n",
    "            variable_summary_light(biases_d)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights_d) + biases_d\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        with tf.name_scope('batch_norm'):\n",
    "            bn = tf.layers.batch_normalization(preactivate, training = True)\n",
    "            tf.summary.histogram('batch_nomralization', bn)\n",
    "        activations = tf.nn.relu(bn)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return(activations, weights_d, biases_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "(encoder_op, weights_b, bias_b, latent) = bottleneck(X, num_input, num_hidden_1)\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(encoder_op, keep_prob)\n",
    "\n",
    "(decoder_op, weights_d_1, bias_d_1)= decoder(dropped, num_hidden_1, num_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "mse = tf.squared_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = tf.nn.l2_loss(weights_b)\n",
    "regularizer += tf.nn.l2_loss(weights_d_1)\n",
    "\n",
    "loss = tf.reduce_sum(mse + lambd * regularizer)/(num_input * batch_size * 2)\n",
    "\n",
    "#Replace learning_rate by learning to get an adaptative learning rate\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary events for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'loss' event\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# Add the 'accuracy' event\n",
    "# tf.summary.scalar(\"accuracy\", acc)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Training the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss: 14.097762\n",
      "Estimated remaining time =  30.93662929534912 s\n",
      "\n",
      "Step 100: Minibatch Loss: 0.029546\n",
      "Estimated remaining time =  13.303734302520754 s\n",
      "\n",
      "Step 200: Minibatch Loss: 0.033417\n",
      "Estimated remaining time =  6.367184519767761 s\n",
      "\n",
      "Step 300: Minibatch Loss: 0.031314\n",
      "Estimated remaining time =  0.0 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "costs = []\n",
    "\n",
    "init_learn =  0.001\n",
    "inter_learn = 0.0001\n",
    "final_learn = 0.00001\n",
    "\n",
    "learn = init_learn\n",
    "\n",
    "# Prepare the summary file for TensorBoard\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "dropout = .9\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    total_batch = int(len(train_set)/batch_size)\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        mini_batch_x, _ = next_batch(batch_size,train_set,train_set)\n",
    "\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        if i==2000:\n",
    "            learn = inter_learn\n",
    "        elif i==4000:\n",
    "            learn = final_learn\n",
    "\n",
    "        _, l, summary = sess.run([optimizer, loss, merged_summary_op], \n",
    "                    feed_dict={X: mini_batch_x, learning : learn,  keep_prob : dropout})\n",
    "        costs.append(l)\n",
    "        \n",
    "    # Write on the log file the datas for TensorBoard / First: summary, Second: overall step\n",
    "    summary_writer.add_summary(summary, i)\n",
    "    \n",
    "    # Display logs per step\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "        curr_time = time()\n",
    "        elapsed_time = curr_time-start_time\n",
    "        print('Estimated remaining time = ', elapsed_time / i * (num_steps-i),'s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Testing the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on n batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Loss:  0.02555535\n",
      "Minibatch Loss:  0.031010203\n",
      "Minibatch Loss:  0.028807899\n",
      "Minibatch Loss:  0.023870215\n",
      "Minibatch Loss:  0.026240176\n",
      "Minibatch Loss:  0.025312988\n",
      "Minibatch Loss:  0.021103423\n",
      "Minibatch Loss:  0.023644065\n",
      "Minibatch Loss:  0.030228525\n",
      "Minibatch Loss:  0.026268454\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "    batch_x, _ = next_batch(batch_size,test_set,test_set)\n",
    "    l , p = sess.run([loss,y_pred] , feed_dict={X: batch_x,  keep_prob: 1})\n",
    "    print('Minibatch Loss: ', l)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the full test set for latent plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_data_array = np.asarray(test_set)\n",
    "test_data_tickers = []\n",
    "for i in range(test_data_array):\n",
    "    ticker = find_ticker_batch(test_data_array[i], data_normalized)\n",
    "    test_data_tickers.append(ticker)\n",
    "\n",
    "pred = sess.run([latent], feed_dict={X: test_data_array, keep_prob: 1})\n",
    "pred = np.asarray(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_data_array[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs/autoencoder1/\"+date+\"/\" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\\n\\n\")\n",
    "print(\"Or Run the command line:\\n\" \\\n",
    "          \"--> python -m tensorboard.main --logdir=C:\\\\tmp\\\\tensorflow_logs\\\\autoencoder1\\\\\"+date+\"\\\\\" \\\n",
    "          \"\\nThen open http://desktop-p6qj80l:6006/ into your web browser, or anything the command tells you to type\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tensorboard --logdir=/tmp/tensorflow_logs/autoencoder1/1519078121/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(pred, data):\n",
    "    minimum = np.min(data)\n",
    "    maximum = np.max(data)\n",
    "    \n",
    "    unnorm = pred * (maximum - minimum) + minimum\n",
    "    return(unnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the predicted values\n",
    "def plot_predict(batch):\n",
    "    batch_size = len(batch)\n",
    "    unnormalized_p = {}\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        (ticker, index) = find_ticker_batch(batch[j], data_normalized)\n",
    "        unnormalized_p[j] = unnormalize(p[j], data[j])\n",
    "    \n",
    "    for j in unnormalized_p.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(unnormalized_p[j] ,'r')\n",
    "        plt.hold\n",
    "        plt.plot(data[j] ,'b')\n",
    "        plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
